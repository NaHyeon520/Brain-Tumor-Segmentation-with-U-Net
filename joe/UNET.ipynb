{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2359e9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "#import cv2 as cv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch, gc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import relu, sigmoid\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8090192e",
   "metadata": {},
   "source": [
    "# Load data and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "140b2518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>images</th>\n",
       "      <th>masks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./data/kaggle_3m\\TCGA_CS_4941_19960909\\TCGA_CS...</td>\n",
       "      <td>./data/kaggle_3m\\TCGA_CS_4941_19960909\\TCGA_CS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./data/kaggle_3m\\TCGA_CS_4941_19960909\\TCGA_CS...</td>\n",
       "      <td>./data/kaggle_3m\\TCGA_CS_4941_19960909\\TCGA_CS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./data/kaggle_3m\\TCGA_CS_4941_19960909\\TCGA_CS...</td>\n",
       "      <td>./data/kaggle_3m\\TCGA_CS_4941_19960909\\TCGA_CS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./data/kaggle_3m\\TCGA_CS_4941_19960909\\TCGA_CS...</td>\n",
       "      <td>./data/kaggle_3m\\TCGA_CS_4941_19960909\\TCGA_CS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./data/kaggle_3m\\TCGA_CS_4941_19960909\\TCGA_CS...</td>\n",
       "      <td>./data/kaggle_3m\\TCGA_CS_4941_19960909\\TCGA_CS...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              images  \\\n",
       "0  ./data/kaggle_3m\\TCGA_CS_4941_19960909\\TCGA_CS...   \n",
       "1  ./data/kaggle_3m\\TCGA_CS_4941_19960909\\TCGA_CS...   \n",
       "2  ./data/kaggle_3m\\TCGA_CS_4941_19960909\\TCGA_CS...   \n",
       "3  ./data/kaggle_3m\\TCGA_CS_4941_19960909\\TCGA_CS...   \n",
       "4  ./data/kaggle_3m\\TCGA_CS_4941_19960909\\TCGA_CS...   \n",
       "\n",
       "                                               masks  \n",
       "0  ./data/kaggle_3m\\TCGA_CS_4941_19960909\\TCGA_CS...  \n",
       "1  ./data/kaggle_3m\\TCGA_CS_4941_19960909\\TCGA_CS...  \n",
       "2  ./data/kaggle_3m\\TCGA_CS_4941_19960909\\TCGA_CS...  \n",
       "3  ./data/kaggle_3m\\TCGA_CS_4941_19960909\\TCGA_CS...  \n",
       "4  ./data/kaggle_3m\\TCGA_CS_4941_19960909\\TCGA_CS...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Dataframe\n",
    "def create_df(data_dir):\n",
    "    images_paths = []\n",
    "    masks_paths = glob(f'{data_dir}/*/*_mask*')\n",
    "    for i in masks_paths:\n",
    "        images_paths.append(i.replace('_mask', ''))\n",
    "    df = pd.DataFrame(data= {'images': images_paths, 'masks': masks_paths})\n",
    "    return df\n",
    "\n",
    "data_dir = './data/kaggle_3m'\n",
    "df = create_df(data_dir)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d349a5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to display images and masks\n",
    "# def show_images(images, masks):\n",
    "#     plt.figure(figsize=(12, 12))\n",
    "#     for i in range(25):\n",
    "#         plt.subplot(5, 5, i + 1)\n",
    "\n",
    "#         img_path = images[i]\n",
    "#         mask_path = masks[i]\n",
    "\n",
    "#         # Read and convert image to RGB\n",
    "#         image = cv.imread(img_path)\n",
    "#         image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
    "\n",
    "#         # Read the mask\n",
    "#         mask = cv.imread(mask_path, cv.IMREAD_GRAYSCALE) \n",
    "\n",
    "#         # Display the image\n",
    "#         plt.imshow(image)\n",
    "#         plt.imshow(mask, cmap=\"jet\", alpha=0.4) \n",
    "#         plt.axis(\"off\")\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "# show_images(list(df[\"images\"]), list(df[\"masks\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfccd466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3929 images successfully.\n",
      "Loaded 3929 images successfully.\n",
      "Final tensor shape: torch.Size([3929, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "class MaskBinarize:\n",
    "    def __call__(self, msk):\n",
    "        msk = (msk > 0.5).float()  \n",
    "        return msk\n",
    "    \n",
    "def img_to_tensor(df, mask=False):#, image_size=(224, 224)):\n",
    "    if mask:\n",
    "        transform = transforms.Compose([\n",
    "            #transforms.Resize(image_size),  \n",
    "            transforms.ToTensor(),          \n",
    "            MaskBinarize()\n",
    "        ])\n",
    "    else:\n",
    "        transform = transforms.Compose([\n",
    "            #transforms.Resize(image_size),  \n",
    "            transforms.ToTensor(),          \n",
    "            #MaskBinarize()\n",
    "        ])\n",
    "    \n",
    "    image_tensors = []\n",
    "    \n",
    "    for img_path in df:\n",
    "        try:\n",
    "            if mask:\n",
    "                img = Image.open(img_path).convert(\"L\")  # grayscale transform\n",
    "                img_tensor = transform(img)  # to tensor\n",
    "                image_tensors.append(img_tensor)\n",
    "            else:\n",
    "                img = Image.open(img_path).convert(\"RGB\")  # RGB transform\n",
    "                img_tensor = transform(img)  # to tensor\n",
    "                image_tensors.append(img_tensor)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "\n",
    "    if image_tensors:\n",
    "        # (batch, C, H, W)\n",
    "        image_tensors = torch.stack(image_tensors)\n",
    "        print(f\"Loaded {len(image_tensors)} images successfully.\")\n",
    "        return image_tensors\n",
    "    else:\n",
    "        print(\"No valid images were loaded.\")\n",
    "        return None\n",
    "\n",
    "image_tensors = img_to_tensor(df[\"images\"])\n",
    "mask_tensors = img_to_tensor(df[\"masks\"], mask=True)\n",
    "\n",
    "if image_tensors is not None:\n",
    "    print(f\"Final tensor shape: {image_tensors.shape}\")  # (N, C, H, W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3aa66695",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np_img = mask_tensors[6].permute(1, 2, 0).numpy()  # shape: [H, W, 3]\n",
    "\n",
    "# print test\n",
    "plt.imshow(np_img)\n",
    "plt.axis('off')\n",
    "plt.title(\"Tensor Image (0~1)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d58a0448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data split: 80/10/10\n",
    "train_img, test_img, train_mask, test_mask = train_test_split(image_tensors, mask_tensors, test_size = 0.1)\n",
    "train_img, val_img, train_mask, val_mask = train_test_split(train_img, train_mask, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "248f2a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(train_img, train_mask)\n",
    "val_dataset = TensorDataset(val_img, val_mask)\n",
    "test_dataset = TensorDataset(test_img, test_mask)\n",
    "\n",
    "trainloader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
    "valloader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
    "testloader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408cbeab",
   "metadata": {},
   "source": [
    "# Model design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272ef53c",
   "metadata": {},
   "source": [
    "In this section, we will design a UNet architecture to show the usage of this data for image segmentation predictive modeling. Here we can see the process of designing a model architecture using Pytorch, along with a custom loss function that will result in optimal performance for this model. Finally, we will train the model over 200 epochs using patience, a concept that we will elaborate on below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bfb26e",
   "metadata": {},
   "source": [
    "First, let's define some hyperparameters for the models architecture and training. These hyperparameters are tuned to obtain optimal performance. Try changing some values and see how the model's performance changes.\n",
    "Let's take a closer look at some of these hyperparameters:\n",
    "- Kernel Size - This is how big the \"sliding window\" of the UNet's convolution layers is. A size of 3 will result in a 3x3xc kernel that takes a weighted average of the corresponding pixel values.\n",
    "- Stride - This is how large the step-size of the kernel is. A stride of 1 means the kernel will move 1 pixel at a time left-to-right and top-to-bottom.\n",
    "- Padding - This is how many pixels we will zero-pad the image with. Having padding allows us to avoid losing information at the boundaries and control image dimensionality shrinkage during convolution.\n",
    "- Number of Epochs - This will control the maximum number of training iterations (over ALL training data) will be completed to find the best possible model parameters.\n",
    "- Patience - This allows us to perform early-stopping if the model fails to improve over a specified number of epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff448e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = './models'\n",
    "\n",
    "kernel_size = 3\n",
    "# The stride is ho\n",
    "stride = 1\n",
    "padding = 1\n",
    "\n",
    "num_epochs = 1\n",
    "patience = 5\n",
    "\n",
    "bce_weight = 0.5 # Coefficient for Binary Cross Entropy loss in loss function\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018ee924",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "\n",
    "Now, we will construct the model. Let's take a closer look at how a UNet works.\n",
    "\n",
    "A UNet is an image segmentation framework that makes predictions in a two step Encoding-Decoding process.\n",
    "\n",
    "At each encoding step, the tensor will be downsampled and the number of channels will increased. This is done by using two convolution layers followed by a max-pooling layer.\n",
    "Max-pooling is a process that downscales a 2D tensor by taking the maximum value within a 2x2 window as the representative value for that group.\n",
    "\n",
    "Next, the tensor will be upsampled in an inverted process to encoding through several steps of decoding. Each docoder step starts by using transposed convolution followed by two convolution layers. The number of channels gradually decreases as the image reverts to it's original shape. \n",
    "\n",
    "To complete a forward pass during training, the Encoder-Decoder steps must be combined in a way that preserves the information of the original image. To do this, we concatonate the the encoder outputs to their corresponfding decoder counterparts after transposed convolution occurs. \n",
    "\n",
    "Through this process, a final output image is produced as the predicted mask for the MRI brain scan. A sigmoid activation is applied to this output to ensure each predicted pixel value is between 0-1, making binary classification simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a9fff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, kernel_size, stride, padding):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder - Downsample input images using max-pooling (256x256x3 images)\n",
    "        self.e11 = nn.Conv2d(3, 32, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.e12 = nn.Conv2d(32, 32, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.e21 = nn.Conv2d(32, 64, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.e22 = nn.Conv2d(64, 64, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.e31 = nn.Conv2d(64, 128, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.e32 = nn.Conv2d(128, 128, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.e41 = nn.Conv2d(128, 256, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.e42 = nn.Conv2d(256, 256, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.maxpool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.e51 = nn.Conv2d(256, 512, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.e52 = nn.Conv2d(512, 512, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "\n",
    "\n",
    "        # Decoder - Upsampling using transposed convolution\n",
    "        self.upconv1 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.d11 = nn.Conv2d(512, 256, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.d12 = nn.Conv2d(256, 256, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.d21 = nn.Conv2d(256, 128, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.d22 = nn.Conv2d(128, 128, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "\n",
    "        self.upconv3 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.d31 = nn.Conv2d(128, 64, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.d32 = nn.Conv2d(64, 64, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)\n",
    "        self.d41 = nn.Conv2d(64, 32, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.d42 = nn.Conv2d(32, 32, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "\n",
    "\n",
    "        # Output\n",
    "        self.output = nn.Conv2d(32, 1, kernel_size=1)\n",
    "\n",
    "\n",
    "    # Input processing through the model\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        e11 = relu(self.e11(x))\n",
    "        e12 = relu(self.e12(e11))\n",
    "        maxpool1 = self.maxpool1(e12)\n",
    "\n",
    "        e21 = relu(self.e21(maxpool1))\n",
    "        e22 = relu(self.e22(e21))\n",
    "        maxpool2 = self.maxpool2(e22)\n",
    "\n",
    "        e31 = relu(self.e31(maxpool2))\n",
    "        e32 = relu(self.e32(e31))\n",
    "        maxpool3 = self.maxpool3(e32)\n",
    "\n",
    "        e41 = relu(self.e41(maxpool3))\n",
    "        e42 = relu(self.e42(e41))\n",
    "        maxpool4 = self.maxpool4(e42)\n",
    "\n",
    "        e51 = relu(self.e51(maxpool4))\n",
    "        e52 = relu(self.e52(e51))\n",
    "\n",
    "        #Decoder\n",
    "        upconv1 = self.upconv1(e52)\n",
    "        upconv1 = torch.cat([upconv1, e42], dim=1)\n",
    "        d11 = self.d11(upconv1)\n",
    "        d12 = self.d12(d11)\n",
    "\n",
    "        upconv2 = self.upconv2(e42)\n",
    "        upconv2 = torch.cat([upconv2, e32], dim=1)\n",
    "        d21 = self.d21(upconv2)\n",
    "        d22 = self.d22(d21)\n",
    "\n",
    "        upconv3 = self.upconv3(d22)\n",
    "        upconv3 = torch.cat([upconv3, e22], dim=1)\n",
    "        d31 = self.d31(upconv3)\n",
    "        d32 = self.d32(d31)\n",
    "\n",
    "        upconv4 = self.upconv4(d32)\n",
    "        upconv4 = torch.cat([upconv4, e12], dim=1)\n",
    "        d41 = self.d41(upconv4)\n",
    "        d42 = self.d42(d41)\n",
    "\n",
    "        out = self.output(d42)\n",
    "        out = sigmoid(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c443663b",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "\n",
    "Binary Cross Entropy Loss is a common loss function used for binary classification. In the case of the MRI brain scan image dataset, BCE loss is allows us to train the model for predicting individual pixels as 0 or 1, penalizing incorrect predictions.\n",
    "\n",
    "Although BCE loss is useful, it does have limitations for this dataset, especially considering class imbalances with most pixels being 0 (no tumor) in these images. To account for this, we will design a custom loss function that incorperates Dice Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e106c238",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1e-6):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "    \n",
    "    def forward(self, pred, true):\n",
    "        pred = pred.view(-1)\n",
    "        true = true.view(-1)\n",
    "        intersection = (pred*true).sum()\n",
    "        dice = (2*intersection + self.smooth) / (pred.sum() + true.sum() + self.smooth)\n",
    "        return 1 - dice\n",
    "\n",
    "class BCEDiceLoss(nn.Module):\n",
    "    def __init__(self, bce_weight=0.5):\n",
    "        super().__init__()\n",
    "        self.bce = nn.BCELoss()\n",
    "        self.dice = DiceLoss()\n",
    "        self.bce_weight = bce_weight\n",
    "\n",
    "    def forward(self, pred, true):\n",
    "        bce_loss = self.bce(pred, true)\n",
    "        dice_loss = self.dice(pred, true)\n",
    "        return self.bce_weight * bce_loss + (1-self.bce_weight) * dice_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e55dda76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNet(\n",
       "  (e11): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (e12): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (e21): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (e22): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (e31): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (e32): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (maxpool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (e41): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (e42): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (upconv2): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (d21): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (d22): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (upconv3): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (d31): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (d32): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (upconv4): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (d41): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (d42): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (output): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = UNet(kernel_size, stride, padding)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ef2cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "best_model_state = None\n",
    "best_model_filename = None\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = BCEDiceLoss(bce_weight=0.5)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in trainloader:\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        images, true_mask = batch\n",
    "        images = images.to(device)\n",
    "        true_mask = true_mask.to(device)\n",
    "\n",
    "        pred_mask = model(images)\n",
    "\n",
    "        loss = 0\n",
    "        loss += criterion(pred_mask, true_mask)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        print(torch.cuda.memory_summary())\n",
    "\n",
    "    avg_loss = total_loss / len(trainloader)\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in valloader:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            images, true_mask = batch\n",
    "            images = images.to(device)\n",
    "            true_mask = true_mask.to(device)\n",
    "\n",
    "            pred_mask = model(images)\n",
    "\n",
    "            loss = 0\n",
    "            loss += criterion(pred_mask, true_mask)\n",
    "            val_loss += loss.item()\n",
    "        \n",
    "    avg_val_loss = val_loss / len(valloader)\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} - Train Loss: {avg_loss:.4f} - Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Best model saving & early stopping\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        epochs_no_improve = 0\n",
    "        best_model_state = model.state_dict() \n",
    "        best_model_filename = f\"{folder_path}/best_model_{str(best_val_loss)}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pth\"\n",
    "        print(\"Validation loss improved. Best model updated.\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"No improvement for {epochs_no_improve} epoch(s).\")\n",
    "    \n",
    "    if epochs_no_improve >= patience:\n",
    "        print(\"Early stopping triggered!\")\n",
    "        break\n",
    "\n",
    "# save the best model\n",
    "if best_model_state is not None:\n",
    "    torch.save(best_model_state, best_model_filename)\n",
    "    print(f\"Best model saved to {best_model_filename}\")\n",
    "\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49986aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(best_model_filename, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "pred_mask_all, true_mask_all = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in testloader:\n",
    "        images, true_mask = batch\n",
    "        images = images.to(device)\n",
    "        true_mask = true_mask.to(device)\n",
    "\n",
    "        pred_mask = model(images)\n",
    "\n",
    "        pred_mask_all.append(pred_mask.cpu().numpy())\n",
    "        true_mask_all.append(true_mask.cpu().numpy())\n",
    "                             \n",
    "    preds = np.concatenate(pred_mask_all, axis=0)   # shape: (N, H, W) or (N, 1, H, W)\n",
    "    trues = np.concatenate(true_mask_all, axis=0)        \n",
    "\n",
    "    if preds.ndim == 4 and preds.shape[1] == 1:\n",
    "        preds = preds.squeeze(1)\n",
    "        trues = trues.squeeze(1)\n",
    "\n",
    "    preds  = (preds > 0.5).astype(np.uint8)\n",
    "    trues  = trues.astype(np.uint8)\n",
    "\n",
    "    preds = preds.flatten()\n",
    "    trues = trues.flatten()\n",
    "\n",
    "    tp = np.logical_and(preds==1, trues==1).sum()\n",
    "    fp = np.logical_and(preds==1, trues==0).sum()\n",
    "    fn = np.logical_and(preds==0, trues==1).sum()\n",
    "\n",
    "    smooth = 1e-6\n",
    "    dice = (2*tp + smooth) / (2*tp + fp + fn + smooth)\n",
    "    iou  = (tp + smooth)   / (tp + fp + fn + smooth)\n",
    "    recall = tp / (tp + fn + smooth)\n",
    "\n",
    "                             \n",
    "    # Dice coefficient\n",
    "    # IoU (Jaccard Index)\n",
    "    # Recall - We care a lot about false negatives\n",
    "    metrics = {\n",
    "        'Dice Coefficient': dice,\n",
    "        'IoU': iou,\n",
    "        'Recall': recall\n",
    "        }\n",
    "    \n",
    "    print(\"\\n **Test Results:**\")\n",
    "    for key, value in metrics.items():\n",
    "        print(f\"{key}: {value:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
